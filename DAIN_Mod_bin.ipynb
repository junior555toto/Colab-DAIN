{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/anaconda3/envs/DAIN/bin/python3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name, driver_version, memory.total [MiB]\n",
      "GeForce RTX 2080 Ti, 460.39, 11019 MiB\n",
      "GeForce RTX 2080 Ti, 460.39, 11019 MiB\n",
      "GeForce RTX 2080 Ti, 460.39, 11016 MiB\n",
      "GeForce RTX 2080 Ti, 460.39, 11019 MiB\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --query-gpu=gpu_name,driver_version,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "GeForce RTX 2080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "enKoi0TR2fOD"
   },
   "outputs": [],
   "source": [
    "################# Required Configurations ############################\n",
    "\n",
    "#@markdown # Required Configuration\n",
    "#@markdown Use the values in here to configure what you'd like DAIN to do.\n",
    "\n",
    "#@markdown ## Input file\n",
    "#@markdown Path (relative to the root of your Google Drive) to the input file. For instance, if you save your `example.mkv` file in your Google Drive, inside a `videos` folder, the path would be: `videos/example.mkv`. Currenly videos and gifs are supported.\n",
    "INPUT_FILEPATH = \"DAIN/input.mp4\" #@param{type:\"string\"}\n",
    "\n",
    "#@markdown ## Output file\n",
    "#@markdown Output file path: path (relative to the root of your Google Drive) for the output file. It will also determine the filetype in the destination. `.mp4` is recommended for video input, `.gif` for gif inputs.\n",
    "OUTPUT_FILE_PATH = \"DAIN/output.mp4\" #@param{type:\"string\"}\n",
    "\n",
    "################# Optional configurations ############################\n",
    "\n",
    "#@markdown # Optional Configuration\n",
    "#@markdown Parameters below can be left with their defaults, but feel free to adapt them to your needs.\n",
    "\n",
    "#@markdown ## Target FPS\n",
    "#@markdown  how many frames per second should the result have. This will determine how many intermediate images are interpolated.\n",
    "TARGET_FPS = 6 #@param{type:\"number\"}\n",
    "\n",
    "#@markdown ## Frame input directory\n",
    "#@markdown A path, relative to your GDrive root, where you already have the list of frames in the format 00001.png, 00002.png, etc.\n",
    "FRAME_INPUT_DIR = '/mnt/wni_related_only/Sample/gsmap' #@param{type:\"string\"}\n",
    "\n",
    "#@markdown ## Frame output directory\n",
    "#@markdown A path, relative to your GDrive root, where you want the generated frame.\n",
    "FRAME_OUTPUT_DIR = '/mnt/wni_related_only/Sample_output' #@param{type:\"string\"}\n",
    "\n",
    "#@markdown ## Start Frame\n",
    "#@markdown First frame to consider from the video when processing.\n",
    "START_FRAME = 1 #@param{type:\"number\"}\n",
    "\n",
    "#@markdown ## End Frame\n",
    "#@markdown Last frame to consider from the video when processing. To use the whole video use `-1`.\n",
    "END_FRAME = -1 #@param{type:\"number\"}\n",
    "\n",
    "#@markdown ## Seamless playback\n",
    "#@markdown Creates a seamless loop by using the first frame as last one as well. Set this to True this if loop is intended.\n",
    "SEAMLESS = False #@param{type:\"boolean\"}\n",
    "\n",
    "#@markdown ## Resize hotfix\n",
    "#@markdown DAIN frames are a bit \"shifted / smaller\" compared to original input frames. This can partly be mitigated with resizing DAIN frames to the resolution +2px and cropping the result to the original resoultion with the starting point (1,1). Without this fix, DAIN tends to make \"vibrating\" output and it is pretty noticible with static elements like text.\n",
    "#@markdown\n",
    "#@markdown This hotfix tries to make such effects less visible for a smoother video playback. I do not know what DAINAPP uses as a fix for this problem, but the original does show such behaviour with the default test images. More advanced users can change the interpolation method. The methods cv2.INTER_CUBIC and cv2.INTER_LANCZOS4 are recommended. The current default value is cv2.INTER_LANCZOS4.\n",
    "RESIZE_HOTFIX = True #@param{type:\"boolean\"}\n",
    "\n",
    "#@markdown ## Auto-remove PNG directory\n",
    "#@markdown Auto-delete output PNG dir after ffmpeg video creation. Set this to `False` if you want to keep the PNG files.\n",
    "AUTO_REMOVE = True #@param{type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Interpolation\n",
    "# %shell mkdir -p '{FRAME_OUTPUT_DIR}'\n",
    "# %cd /content/DAIN\n",
    "\n",
    "# !python -W ignore colab_interpolate.py --netName DAIN_slowmotion --time_step {fps/TARGET_FPS} --start_frame 1 --end_frame {frame_count} --frame_input_dir '{FRAME_INPUT_DIR}' --frame_output_dir '{FRAME_OUTPUT_DIR}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--debug] [--netName {DAIN,DAIN_slowmotion}]\n",
      "                             [--datasetName {V,i,m,e,o,_,9,0,K,_,i,n,t,e,r,p} [{V,i,m,e,o,_,9,0,K,_,i,n,t,e,r,p} ...]]\n",
      "                             [--datasetPath DATASETPATH]\n",
      "                             [--dataset_split DATASET_SPLIT] [--seed SEED]\n",
      "                             [--numEpoch NUMEPOCH] [--batch_size BATCH_SIZE]\n",
      "                             [--workers WORKERS] [--channels {1,3}]\n",
      "                             [--filter_size {2,4,5,6,51}] [--lr LR]\n",
      "                             [--rectify_lr RECTIFY_LR] [--save_which {0,1}]\n",
      "                             [--time_step TIME_STEP]\n",
      "                             [--flow_lr_coe FLOW_LR_COE]\n",
      "                             [--occ_lr_coe OCC_LR_COE]\n",
      "                             [--filter_lr_coe FILTER_LR_COE]\n",
      "                             [--ctx_lr_coe CTX_LR_COE]\n",
      "                             [--depth_lr_coe DEPTH_LR_COE]\n",
      "                             [--alpha ALPHA [ALPHA ...]] [--epsilon EPSILON]\n",
      "                             [--weight_decay WEIGHT_DECAY]\n",
      "                             [--patience PATIENCE] [--factor FACTOR]\n",
      "                             [--pretrained SAVED_MODEL] [--no-date]\n",
      "                             [--use_cuda USE_CUDA] [--use_cudnn USE_CUDNN]\n",
      "                             [--dtype {<class 'torch.cuda.FloatTensor'>,<class 'torch.FloatTensor'>}]\n",
      "                             [--uid UID] [--force] [--start_frame START_FRAME]\n",
      "                             [--end_frame END_FRAME]\n",
      "                             [--frame_input_dir FRAME_INPUT_DIR]\n",
      "                             [--frame_output_dir FRAME_OUTPUT_DIR]\n",
      "ipykernel_launcher.py: error: argument --filter_size/-f: invalid int value: '/home/luigisama/.local/share/jupyter/runtime/kernel-fb9f2b2e-4c62-4641-8599-7ddbef6a9ce7.json'\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/DAIN/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2886: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import numpy as np\n",
    "import numpy\n",
    "import networks\n",
    "from my_args import args\n",
    "from imageio import imread, imsave\n",
    "from AverageMeter import  *\n",
    "import shutil\n",
    "import datetime\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "#args settings\n",
    "args.netName = \"DAIN_slowmotion\"\n",
    "args.time_step = 2/6\n",
    "args.frame_input_dir = FRAME_INPUT_DIR\n",
    "args.frame_output_dir = FRAME_OUTPUT_DIR\n",
    "\n",
    "\n",
    "model = networks.__dict__[args.netName](\n",
    "                                    channel = args.channels,\n",
    "                                    filter_size = args.filter_size,\n",
    "                                    timestep = args.time_step,\n",
    "                                    training = False)\n",
    "\n",
    "# --netName DAIN_slowmotion --time_step {fps/TARGET_FPS} --start_frame 1 --end_frame {frame_count} --frame_input_dir '{FRAME_INPUT_DIR}' --frame_output_dir '{FRAME_OUTPUT_DIR}\n",
    "\n",
    "print(args.channels)\n",
    "\n",
    "\n",
    "\n",
    "if args.use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "model_path = './model_weights/best.pth'\n",
    "if not os.path.exists(model_path):\n",
    "    print(\"*****************************************************************\")\n",
    "    print(\"**** We couldn't load any trained weights ***********************\")\n",
    "    print(\"*****************************************************************\")\n",
    "    exit(1)\n",
    "\n",
    "if args.use_cuda:\n",
    "    pretrained_dict = torch.load(model_path)\n",
    "else:\n",
    "    pretrained_dict = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
    "\n",
    "model_dict = model.state_dict()\n",
    "# 1. filter out unnecessary keys\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "# 2. overwrite entries in the existing state dict\n",
    "model_dict.update(pretrained_dict)\n",
    "# 3. load the new state dict\n",
    "model.load_state_dict(model_dict)\n",
    "# 4. release the pretrained dict for saving memory\n",
    "pretrained_dict = []\n",
    "\n",
    "model = model.eval() # deploy mode\n",
    "\n",
    "frames_dir = args.frame_input_dir\n",
    "output_dir = args.frame_output_dir\n",
    "\n",
    "timestep = args.time_step\n",
    "time_offsets = [kk * timestep for kk in range(1, int(1.0 / timestep))]\n",
    "\n",
    "input_frame = args.start_frame - 1\n",
    "loop_timer = AverageMeter()\n",
    "\n",
    "final_frame = args.end_frame\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# we want to have input_frame between (start_frame-1) and (end_frame-2)\n",
    "# this is because at each step we read (frame) and (frame+1)\n",
    "# so the last iteration will actuall be (end_frame-1) and (end_frame)\n",
    "while input_frame < final_frame - 1:\n",
    "    input_frame += 1\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    filename_frame_1 = os.path.join(frames_dir, 'gsmap_nrt_japansub.20180601.0000.bin')\n",
    "    filename_frame_2 = os.path.join(frames_dir, 'gsmap_nrt_japansub.20180601.0100.bin')\n",
    "    \n",
    "#     filename_frame_1 = os.path.join(frames_dir, f'{input_frame:0>5d}.bin')\n",
    "#     filename_frame_2 = os.path.join(frames_dir, f'{input_frame+1:0>5d}.bin')\n",
    "\n",
    "    X0 = torch.from_numpy(np.fromfile(filename_frame_1, dtype=np.float32).reshape(1,279,319) / 255.0).type(args.dtype)\n",
    "    X1 = torch.from_numpy(np.fromfile(filename_frame_2, dtype=np.float32).reshape(1,279,319) / 255.0).type(args.dtype)\n",
    "\n",
    "#     X0 = torch.from_numpy(np.transpose(imread(filename_frame_1), (2,0,1)).astype(\"float32\") / 255.0).type(args.dtype)\n",
    "#     X1 = torch.from_numpy(np.transpose(imread(filename_frame_2), (2,0,1)).astype(\"float32\") / 255.0).type(args.dtype)\n",
    "\n",
    "    assert (X0.size(1) == X1.size(1))\n",
    "    assert (X0.size(2) == X1.size(2))\n",
    "\n",
    "    intWidth = X0.size(2)\n",
    "    intHeight = X0.size(1)\n",
    "    channels = X0.size(0)\n",
    "    if not channels == 3:\n",
    "        print(f\"Skipping {filename_frame_1}-{filename_frame_2} -- expected 3 color channels but found {channels}.\")\n",
    "        continue\n",
    "\n",
    "    if intWidth != ((intWidth >> 7) << 7):\n",
    "        intWidth_pad = (((intWidth >> 7) + 1) << 7)  # more than necessary\n",
    "        intPaddingLeft = int((intWidth_pad - intWidth) / 2)\n",
    "        intPaddingRight = intWidth_pad - intWidth - intPaddingLeft\n",
    "    else:\n",
    "        intPaddingLeft = 32\n",
    "        intPaddingRight= 32\n",
    "\n",
    "    if intHeight != ((intHeight >> 7) << 7):\n",
    "        intHeight_pad = (((intHeight >> 7) + 1) << 7)  # more than necessary\n",
    "        intPaddingTop = int((intHeight_pad - intHeight) / 2)\n",
    "        intPaddingBottom = intHeight_pad - intHeight - intPaddingTop\n",
    "    else:\n",
    "        intPaddingTop = 32\n",
    "        intPaddingBottom = 32\n",
    "\n",
    "    pader = torch.nn.ReplicationPad2d([intPaddingLeft, intPaddingRight, intPaddingTop, intPaddingBottom])\n",
    "\n",
    "    X0 = Variable(torch.unsqueeze(X0,0))\n",
    "    X1 = Variable(torch.unsqueeze(X1,0))\n",
    "    X0 = pader(X0)\n",
    "    X1 = pader(X1)\n",
    "\n",
    "    if args.use_cuda:\n",
    "        X0 = X0.cuda()\n",
    "        X1 = X1.cuda()\n",
    "\n",
    "    y_s, offset, filter = model(torch.stack((X0, X1),dim = 0))\n",
    "    y_ = y_s[args.save_which]\n",
    "\n",
    "    if args.use_cuda:\n",
    "        X0 = X0.data.cpu().numpy()\n",
    "        if not isinstance(y_, list):\n",
    "            y_ = y_.data.cpu().numpy()\n",
    "        else:\n",
    "            y_ = [item.data.cpu().numpy() for item in y_]\n",
    "        offset = [offset_i.data.cpu().numpy() for offset_i in offset]\n",
    "        filter = [filter_i.data.cpu().numpy() for filter_i in filter]  if filter[0] is not None else None\n",
    "        X1 = X1.data.cpu().numpy()\n",
    "    else:\n",
    "        X0 = X0.data.numpy()\n",
    "        if not isinstance(y_, list):\n",
    "            y_ = y_.data.numpy()\n",
    "        else:\n",
    "            y_ = [item.data.numpy() for item in y_]\n",
    "        offset = [offset_i.data.numpy() for offset_i in offset]\n",
    "        filter = [filter_i.data.numpy() for filter_i in filter]\n",
    "        X1 = X1.data.numpy()\n",
    "\n",
    "    X0 = np.transpose(255.0 * X0.clip(0,1.0)[0, :, intPaddingTop:intPaddingTop+intHeight, intPaddingLeft: intPaddingLeft+intWidth], (1, 2, 0))\n",
    "    y_ = [np.transpose(255.0 * item.clip(0,1.0)[0, :, intPaddingTop:intPaddingTop+intHeight,\n",
    "                                intPaddingLeft:intPaddingLeft+intWidth], (1, 2, 0)) for item in y_]\n",
    "    offset = [np.transpose(offset_i[0, :, intPaddingTop:intPaddingTop+intHeight, intPaddingLeft: intPaddingLeft+intWidth], (1, 2, 0)) for offset_i in offset]\n",
    "    filter = [np.transpose(\n",
    "        filter_i[0, :, intPaddingTop:intPaddingTop + intHeight, intPaddingLeft: intPaddingLeft + intWidth],\n",
    "        (1, 2, 0)) for filter_i in filter]  if filter is not None else None\n",
    "    X1 = np.transpose(255.0 * X1.clip(0,1.0)[0, :, intPaddingTop:intPaddingTop+intHeight, intPaddingLeft: intPaddingLeft+intWidth], (1, 2, 0))\n",
    "\n",
    "    interpolated_frame_number = 0\n",
    "    shutil.copy(filename_frame_1, os.path.join(output_dir, f\"{input_frame:0>5d}{interpolated_frame_number:0>3d}.bin\"))\n",
    "    for item, time_offset in zip(y_, time_offsets):\n",
    "        interpolated_frame_number += 1\n",
    "        output_frame_file_path = os.path.join(output_dir, f\"{input_frame:0>5d}{interpolated_frame_number:0>3d}.bin\")\n",
    "        \n",
    "        item.astype('float32').tofile(output_frame_file_path)\n",
    "        \n",
    "#         imsave(output_frame_file_path, np.round(item).astype(numpy.uint8))\n",
    "        # ให้เปลัี่ยนเป็น Save to file ด้วย numpy, ไม่ต้องใช้ np.round , astype เป็น float32 \n",
    "\n",
    "    end_time = time.time()\n",
    "    loop_timer.update(end_time - start_time)\n",
    "\n",
    "    frames_left = final_frame - input_frame\n",
    "    estimated_seconds_left = frames_left * loop_timer.avg\n",
    "    estimated_time_left = datetime.timedelta(seconds=estimated_seconds_left)\n",
    "    print(f\"****** Processed frame {input_frame} | Time per frame (avg): {loop_timer.avg:2.2f}s | Time left: {estimated_time_left} ******************\" )\n",
    "\n",
    "# Copying last frame\n",
    "last_frame_filename = os.path.join(frames_dir, str(str(final_frame).zfill(5))+'.bin')\n",
    "shutil.copy(last_frame_filename, os.path.join(output_dir, f\"{final_frame:0>5d}{0:0>3d}.bin\"))\n",
    "\n",
    "print(\"Finished processing images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding DAIN Frames, upscaling and cropping to match original\n",
    "# %cd {FRAME_OUTPUT_DIR}\n",
    "\n",
    "if (RESIZE_HOTFIX):\n",
    "  images = []\n",
    "  for filename in os.listdir(FRAME_OUTPUT_DIR):\n",
    "    img = cv2.imread(os.path.join(FRAME_OUTPUT_DIR, filename))\n",
    "    filename = os.path.splitext(filename)[0]\n",
    "    if(not filename.endswith('0')):\n",
    "      dimensions = (img.shape[1]+2, img.shape[0]+2)\n",
    "      resized = cv2.resize(img, dimensions, interpolation=cv2.INTER_LANCZOS4)\n",
    "      crop = resized[1:(dimensions[1]-1), 1:(dimensions[0]-1)]\n",
    "      cv2.imwrite(f\"{filename}.png\", crop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 DAIN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
